<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Edwardyam.GitHub.io : Welcome">

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Edwardyam.GitHub.io</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/EdwardYam">View on GitHub</a>

          <h1 id="project_title">Edwardyam.GitHub.io</h1>
          <h2 id="project_tagline">Welcome to my github blog</h2>

        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

        <h3>
            <a id="creating-pages-manually" class="anchor" href="#creating-pages-manually" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>统计语言模型简述
        </h3>

          <p>
          最近在学习机器学习、自然语言处理相关知识。就我目前接触的范围来说，这两方面数学占的比重还是极大的。
          机器学习的课程我推荐coursera上的machine learning课程。而自然语言处理的大致了解我觉得可以从“数学之美”这本书开始着手。
          本文重点对我看了数学之美的几个章节的知识点进行总结归纳。<br/><br/>

          在上世纪末基于规则的自然语言处理和基于统计的自然语言处理处于相持阶段。两方面学者的争吵在Google公司的基于统计方法的翻译系统全面超过基于规则方法的SysTran翻译系统后，正式终结。基于规则的自然语言处理其实会面临许多问题，其中一个问题就是基于常识的处理。在1966年，人工智能专家明斯基用一个简单的例子阐明了这一问题。“The pen is in the box”（这支钢笔在盒子里面），"The box is in the pen"（这个盒子在栅栏里面）。这个问题的关键在于pen所表达的意思的确定。这里既不能从上下文获取信息也无法从句子本身确定，需要的是钢笔通常比盒子小而栅栏比盒子大的常识。这里的翻译需要基于这样的常识对pen的意思作出选择。若仍采用基于规则的自然语言处理方法，要处理这样的问题不但难度大而且会大大增加问题的规模。<br/><br/>

          在经过漫长的摸索之后，人们最终发现并确定使用基于统计的自然语言处理方法。这样的方法则基于语料库来描述语言的规律。人工智能领域的另一位学者贾里尼克提出了一个极为简单但又有效的思想：一个句子是否合理，就看看它的可能性大小如何。假设S为一个句子，它由一连串特定排列顺序的词w1,w2,w3...wn组成。句子S在语料库中出现的概率P(S)=P(w1,w2,w3...wn)。根据条件概率公式P(w1,w2,w3...wn)=P(w1)*P(w2|w1)*p(w3|w1,w2)...P(wn|w1,w2...w(n-1))。<br/><br/>

          可见，这样虽然将问题简化，但随着一个句子包含的词变多，计算难度以及计算规模会越来越大。19世纪末20世纪初，俄罗斯数学家马尔可夫给这种问题添加了一个假设：任意一个词wi出现的概率仅与第w(i-1)个词有关与再之前的词没关。这样计算P(S)的方法则会简单许多：P(w1,w2,w3...wn)=P(w1)*P(w2|w1)*p(w3|w2)...P(wn|w(n-1))，这就是统计语言模型中的二元模型。<br/><br/>

          而在实际使用中，往往会把这个假设进一步泛化，即假设任意一个词wi出现的概率仅与第w(i-1)和第w(i-2)个词有关与再之前的词没关，即三元模型（另外一元模型即假设每个词的出现频率与之前的词没有任何关系，即上下文无关）。而极少会有公司再将三元模型往上加到四元模型，一方面，从二元模型到三元模型，模型的效果提升比较大，而从三元到四元效果并不那么明显，而且计算机资源消耗大了许多，另外，即便一直提升模型的元的数量，也无法涵盖自然语言中的所有情况。因为自然语言中，相关性的跨度可能是跨句甚至是跨段，这样的话即使一直提高模型中元的数量也无法涵盖这些情况。<br/><br/>

          除此之外，还有一些细节问题需要处理。假设某个组合在语料库中一次不出现是否就认为这个组合是不符合语法规则的？如果一个词仅出现一次的唯一一次出现刚好和要检索的组合一致，是否就认为这个组合正确率是百分之百？我们可以把出现频度当作概率来看是在足够大的数据量的支持下作出的结论，如果出现次数不够大，就不能够再简单的把语料库中出现频率当作概率。这个时候词库的整体不再是原本的样子，词库由两部分组成，看得见的事情的部分以及未看见的事情的部分。基于这种思想的话二元模型的概率如下：<br/><br/>

          P(wi|w(i-1))=) f(wi|w(i-1)，当二元组(wi,w(i-1))大于等于某个值T时
          P(wi|w(i-1))= ft(wi|w(i-1)，当二元组(wi,w(i-1))小于某个值T并大于0时，ft会对频度进行一定程度的下调，越是不可能的事件，下调的幅度就越大
          P(wi|w(i-1))=Q(w(i-1))*f(wi)，在其它条件下（原文这样描述，个人认为就是语料库中并未出现该词组的情况）函数Q描述如下<br/><br/>

          这样就处理了小概率事件以及语料库未能覆盖的词组的情况，并将概率进行平缓化，使得模型更为准确。
          </p>
        </section>
    </div>
    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

  </body>
</html>
